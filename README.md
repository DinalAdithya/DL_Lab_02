# Deep Learning â€“ Lab 2

## Overview
This lab focuses on understanding and experimenting with neural networks and deep learning concepts. The lab exercises include exploring backpropagation, tuning hidden layer sizes, and working with the MNIST dataset using Multi-Layer Perceptrons (MLPs).

The objectives are:
1. Understand and experiment with the Backpropagation algorithm.
2. Explore the effect of the number of hidden nodes on neural network performance.
3. Apply MLPs to the MNIST dataset and improve model accuracy using hyperparameter tuning and regularization.

---

## Files in this repository
- `notebooks/ex1.ipynb` - Notebook demonstrating backpropagation.  
- `notebooks/ex2.ipynb` - Sample neural network with exercises to tune hidden layer sizes.  
- `notebooks/ex3.ipynb` - MLP applied on MNIST dataset with hyperparameter tuning and regularization.  
- `data/image.png` - Image used in Backprop.ipynb.  
- `data/planar_utils.py` and `data/testCases.py` - Utility and test files for ex2.ipynb.
